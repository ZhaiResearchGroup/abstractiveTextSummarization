{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Summarization with a Sequence to Sequence Network and Attention on Language Model\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a single vector, and a decoder network unfolds that vector into a new sequence. \n",
    "\n",
    "To improve upon this model we'll use an [attention mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder learn to focus over a specific range of the input sequence. This is initially designed for data-driven language translation, in which case the length of input and output sequence is approximately similar. For text summarization task, in which case the output sequence is much shorter then the input, the model didn't perform that well. Some other tricks should be explored soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequence to Sequence model\n",
    "\n",
    "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **encoder** and **decoder**. The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. The decoder uses this context vector to produce a sequence of outputs one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Mechanism\n",
    "\n",
    "The fixed-length vector carries the burden of encoding the the entire \"meaning\" of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. Imagine two nearly identical sentences, twenty words long, with only one word different. Both the encoders and decoders must be nuanced enough to represent that change as a very slightly different point in space.\n",
    "\n",
    "The **attention mechanism** [introduced by Bahdanau et al.](https://arxiv.org/abs/1409.0473) addresses this by giving the decoder a way to \"pay attention\" to parts of the input, rather than relying on a single vector. For every step the decoder can select a different part of the input sentence to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "'''\n",
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
    "from masked_cross_entropy import *\n",
    "torch.version.cuda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language. On other tutorials,  they adopt an initial embedding matrix built from [GloVe](http://nlp.stanford.edu/projects/glove/), which can be tested afterwards.\n",
    "\n",
    "### Sentence Embeddings\n",
    "We'll use the gensim's library of pretrained sentence-to-vector models originally trained on the AP News dataset to use as a sentence-level analogue to semantic word embeddings. This is used as the second layer of the hierarchical attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "sentence_size = 100\n",
    "\n",
    "sent2vec = gensim.models.doc2vec.Doc2Vec.load(\"/Data/apnews_model/apnews_sen_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [ 0.13891193 -0.05098223 -0.10987     0.08467694  0.09408356  0.02826136\n",
      " -0.00210058  0.1921066   0.06350647 -0.09396907 -0.07495762 -0.04290688\n",
      " -0.03049732  0.11854893  0.05515203 -0.09074111 -0.16444759  0.05689705\n",
      " -0.14535739  0.10919423 -0.25108767  0.2698584   0.00403622  0.06346805\n",
      " -0.07443989 -0.02245832 -0.06924523 -0.15734594  0.00238275 -0.00315631\n",
      " -0.09173147 -0.02689783  0.31112048  0.14448425 -0.01530252 -0.13690203\n",
      "  0.06994422 -0.0157758   0.02470064 -0.01708556 -0.10394131 -0.04463577\n",
      "  0.25290135 -0.06923533  0.08974954  0.10165834  0.03351844  0.0086526\n",
      "  0.02757375 -0.29573864  0.26033178  0.21193527 -0.00110585 -0.16219625\n",
      " -0.19045095 -0.02979052 -0.01942467 -0.09300861  0.10371514  0.10666463\n",
      "  0.03849122  0.01399612  0.10872983 -0.04527485  0.1646421  -0.13469465\n",
      " -0.16336347 -0.21305454 -0.00572637 -0.24163203 -0.13183752 -0.10064763\n",
      "  0.05860639  0.05062276 -0.09159366  0.03078913 -0.11887912 -0.15038688\n",
      " -0.16159458  0.2840952  -0.00851821  0.13445242  0.18858384 -0.09833488\n",
      " -0.02051114  0.3117034   0.0175563  -0.03369217  0.04611785  0.00751185\n",
      " -0.20792659  0.05461083 -0.00239726  0.30178165 -0.24927068  0.1721526\n",
      "  0.06985214 -0.09921457  0.02817388  0.08578531]\n"
     ]
    }
   ],
   "source": [
    "embedding = sent2vec.infer_vector('Hello my name is friend.')\n",
    "print(len(embedding), embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torchtext.vocab as vocab\n",
    "from torchtext.data import RawField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only for data visualization, does not need to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query Index</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Raw Query</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Sen_Vecs</th>\n",
       "      <th>Sen_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>James Bond.txt</td>\n",
       "      <td>[ ' as of 2018 there have been twenty four fi...</td>\n",
       "      <td>the james bond series focuses on a fictional b...</td>\n",
       "      <td>&lt;sos&gt; this article is about the spy series in ...</td>\n",
       "      <td>&lt;sos&gt; this article is about the spy series in ...</td>\n",
       "      <td>&lt;sos&gt; this article is about the spy series in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Speed.txt</td>\n",
       "      <td>[ ' [ 1 ] the average speed of an object in a...</td>\n",
       "      <td>in everyday use and in kinematics the speed of...</td>\n",
       "      <td>&lt;sos&gt; this article is about the property of mo...</td>\n",
       "      <td>&lt;sos&gt; this article is about the property of mo...</td>\n",
       "      <td>&lt;sos&gt; this article is about the property of mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Official language.txt</td>\n",
       "      <td>[ ' [ 1 ] since \" the means of expression of ...</td>\n",
       "      <td>an official language is a language that is giv...</td>\n",
       "      <td>&lt;sos&gt; an official language is a language that ...</td>\n",
       "      <td>&lt;sos&gt; an official language is a language that ...</td>\n",
       "      <td>&lt;sos&gt; an official language is a language that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Federal Register.txt</td>\n",
       "      <td>[ ' the federal register is compiled by the o...</td>\n",
       "      <td>the federal register ( fr or sometimes fed reg...</td>\n",
       "      <td>&lt;sos&gt; federal register &lt;sos&gt; cover &lt;sos&gt; type ...</td>\n",
       "      <td>&lt;sos&gt; federal register &lt;sos&gt; cover &lt;sos&gt; type ...</td>\n",
       "      <td>&lt;sos&gt; federal register &lt;sos&gt; cover &lt;sos&gt; type ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Flash memory.txt</td>\n",
       "      <td>[ ' although flash memory is technically a ty...</td>\n",
       "      <td>flash memory is an electronic ( solid state ) ...</td>\n",
       "      <td>&lt;sos&gt; for the neuropsychological concept relat...</td>\n",
       "      <td>&lt;sos&gt; for the neuropsychological concept relat...</td>\n",
       "      <td>&lt;sos&gt; for the neuropsychological concept relat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query Index                 Titles  \\\n",
       "0            0         James Bond.txt   \n",
       "1            1              Speed.txt   \n",
       "2            2  Official language.txt   \n",
       "3            3   Federal Register.txt   \n",
       "4            4       Flash memory.txt   \n",
       "\n",
       "                                           Raw Query  \\\n",
       "0   [ ' as of 2018 there have been twenty four fi...   \n",
       "1   [ ' [ 1 ] the average speed of an object in a...   \n",
       "2   [ ' [ 1 ] since \" the means of expression of ...   \n",
       "3   [ ' the federal register is compiled by the o...   \n",
       "4   [ ' although flash memory is technically a ty...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  the james bond series focuses on a fictional b...   \n",
       "1  in everyday use and in kinematics the speed of...   \n",
       "2  an official language is a language that is giv...   \n",
       "3  the federal register ( fr or sometimes fed reg...   \n",
       "4  flash memory is an electronic ( solid state ) ...   \n",
       "\n",
       "                                           Documents  \\\n",
       "0  <sos> this article is about the spy series in ...   \n",
       "1  <sos> this article is about the property of mo...   \n",
       "2  <sos> an official language is a language that ...   \n",
       "3  <sos> federal register <sos> cover <sos> type ...   \n",
       "4  <sos> for the neuropsychological concept relat...   \n",
       "\n",
       "                                            Sen_Vecs  \\\n",
       "0  <sos> this article is about the spy series in ...   \n",
       "1  <sos> this article is about the property of mo...   \n",
       "2  <sos> an official language is a language that ...   \n",
       "3  <sos> federal register <sos> cover <sos> type ...   \n",
       "4  <sos> for the neuropsychological concept relat...   \n",
       "\n",
       "                                            Sen_idxs  \n",
       "0  <sos> this article is about the spy series in ...  \n",
       "1  <sos> this article is about the property of mo...  \n",
       "2  <sos> an official language is a language that ...  \n",
       "3  <sos> federal register <sos> cover <sos> type ...  \n",
       "4  <sos> for the neuropsychological concept relat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# note. this is the wrong dataset right now. (I think)\n",
    "data_df = pd.read_csv('/Data/utiman/wiki_queries12.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1407\n",
       "-0.0518\n",
       "-0.1153\n",
       " 0.0811\n",
       " 0.0930\n",
       "[torch.cuda.FloatTensor of size 5 (GPU 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.FloatTensor(embedding)\n",
    "if USE_CUDA:\n",
    "    tensor = tensor.cuda()\n",
    "tensor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "vectors = sparse.load_npz('/Data/utiman/data/queries_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(batch):#, device, train):\n",
    "    \"\"\" Process a list of examples to create a torch.Tensor.\n",
    "    Pad, numericalize, and postprocess a batch and create a tensor.\n",
    "    Args:\n",
    "        batch (list(object)): A list of object from a batch of examples.\n",
    "    Returns:\n",
    "        torch.autograd.Variable: Processed object given the input\n",
    "            and custom postprocessing Pipeline.\n",
    "    \"\"\"\n",
    "    # vectors_file = sparse.load_npz('../data/queries_matrix.npz')\n",
    "\n",
    "    csr_vec = vectors[batch[0]]\n",
    "    vec = sparse.csr_matrix.todense(csr_vec)\n",
    "    tensor = torch.LongTensor(vec)\n",
    "    \n",
    "    ret_matrix = tensor\n",
    "    \n",
    "    for q_idx in batch[1::]:\n",
    "        #print(q_idx)\n",
    "        csr_vec = vectors[batch[0]]\n",
    "        vec = sparse.csr_matrix.todense(csr_vec)\n",
    "        tensor = torch.LongTensor(vec)\n",
    "        ret_matrix = torch.cat((ret_matrix, tensor),1)\n",
    "    \n",
    "    #if device != -1:\n",
    "    if USE_CUDA:\n",
    "        ret_matrix = ret_matrix.contiguous().cuda()\n",
    "    \n",
    "    return torch.autograd.Variable(ret_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @args:\n",
    "        batch: a list of strings where each string is a document to be converted to sen_vec \n",
    "            representation\n",
    "    @returns: np.array that is (batch_size x num_sens_in_doc x 100) \n",
    "                This array si the sentences embeddings for each sentence \n",
    "                in each document in the batch\n",
    "'''\n",
    "def sen_vec_postprocess(batch):\n",
    "    \n",
    "    # tokenized batch is now a list[list[sentences]]\n",
    "    # split at <sos> tokens\n",
    "    tokenized_batch = [['<sos>' + sen for sen in example.split('<sos>')][1:] for example in batch]\n",
    "\n",
    "    # maximum length of any document (in number of sentences)\n",
    "    max_doc_len = np.max(np.array([len(ex) for ex in tokenized_batch]))\n",
    "    \n",
    "    batch_vec = np.zeros((len(batch), max_doc_len+1, 100))\n",
    "    \n",
    "    for i, example in enumerate(tokenized_batch):\n",
    "        # length of this example tells us how much we need to leave as padding on the end\n",
    "        for j in range(len(example)):\n",
    "            batch_vec[i,:] = np.array(sent2vec.infer_vector(example[j]))\n",
    "#         for j in range(len(example), max_doc_len):\n",
    "#             batch_vec[i,:] = 0\n",
    "\n",
    "    # return as cuda var\n",
    "    tensor = torch.FloatTensor(batch_vec)\n",
    "    print(tensor[:,:,10])\n",
    "    if USE_CUDA:\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @args:\n",
    "        batch: a list of strings where each string is a document to be converted to sen_vec \n",
    "            representation\n",
    "    @returns: np.array that is (batch_size x num_sens_in_doc x 100) \n",
    "                This array si the sentences embeddings for each sentence \n",
    "                in each document in the batch\n",
    "'''\n",
    "def sen_idx_postprocess(batch):\n",
    "    # tokenized batch is now a list[list[sentences]]\n",
    "    # split at <sos> tokens\n",
    "    sentences_batch = [['<sos>' + sen for sen in example.split('<sos>')][1:] for example in batch]\n",
    "\n",
    "    # words batch is now a list[list[words]]\n",
    "    words_batch = [example.split() for example in batch]\n",
    "\n",
    "    # maximum length of any document (in number of words)\n",
    "    max_doc_len = max([len(ex) for ex in words_batch])\n",
    "    \n",
    "    # maximum number of sentences in any document\n",
    "    max_sen_num = max([len(ex) for ex in sentences_batch])\n",
    "\n",
    "    # initialize the array of sentence indices\n",
    "    # we use one more than the maximum to be the zero vector.\n",
    "    # In sentence attentions, this always receives zero weight\n",
    "    sen_idxs = np.ones((len(batch), max_doc_len)) * max_sen_num\n",
    "    \n",
    "    # for ea\n",
    "    for sen_idx, example in enumerate(sentences_batch):\n",
    "        curr = 0\n",
    "        for wd_idx, sent in enumerate(example):\n",
    "            sen_idxs[sen_idx, curr:curr+len(sent.split())] = wd_idx\n",
    "            curr += len(sent.split())\n",
    "\n",
    "    if USE_CUDA:\n",
    "        return torch.autograd.Variable(torch.LongTensor(sen_idxs).cuda())\n",
    "    else:\n",
    "        return torch.autograd.Variable(torch.LongTensor(sen_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     0     0     0     0     0     1     1     1     1     2     2     2\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 25 \n",
       "    2     2     2     3     3     3     3     3     3     3     4     4     4\n",
       "    0     0     0     0     0     0     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 26 to 36 \n",
       "    4     4     4     4     4     4     4     4     4     4     4\n",
       "    1     1     1     2     2     2     2     2     2     2     2\n",
       "[torch.cuda.LongTensor of size 2x37 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"<sos> lorem ipsum dolor sit amet <sos> consectetur adipiscing elit <sos> sed do eiusmod tempor incididunt <sos> ut labore et dolore magna aliqua\",\n",
    "         \"<sos> ut enim ad minim veniam , quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat <sos> Duis aute irure dolor in reprehenderit in voluptate velit <sos> esse cillum dolore eu fugiat nulla pariatur\"]\n",
    "\n",
    "sen_idx_postprocess(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my tokenize:  [['<sos> hello my name is friend']]\n",
      "\n",
      "-0.1111 -0.1111\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "my tokenize:  [['<sos> hello my name is friend']]\n",
      "\n",
      "-0.1218 -0.1218\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input vector should be 1-D.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-861ab50e4a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_vector\u001b[0;34m(u, dtype)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input vector should be 1-D.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input vector should be 1-D."
     ]
    }
   ],
   "source": [
    "batch = [\"<sos> hello my name is friend\"]\n",
    "v = sen_vec_postprocess(batch).squeeze()\n",
    "u = sen_vec_postprocess(batch).squeeze()\n",
    "v = v.data.cpu().numpy()\n",
    "u = u.data.cpu().numpy()\n",
    "\n",
    "from scipy import spatial\n",
    "#print(dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v)))\n",
    "print(spatial.distance.cosine(u,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7fe7a8cfe630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True)\n",
    "QUERY = data.RawField(postprocessing=postprocess) # sequential=False, use_vocab = False,\n",
    "SEN_VEC = data.RawField(postprocessing=sen_vec_postprocess) #, sequential=False, use_vocab = False)\n",
    "SEN_IDX = data.RawField(postprocessing=sen_idx_postprocess)\n",
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "# need to do this so we dont get errors about having too big of a file in a single cell of a csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, = data.TabularDataset.splits(\n",
    "        path='/Data/utiman/', train='wiki_queries12.csv',format='csv', skip_header = True,\n",
    "        fields=[('query_num', QUERY), ('title', TEXT), ('raw_query', TEXT), ('sum', TEXT),\n",
    "                ('story', TEXT), ('sen_vec', SEN_VEC), ('sen_idx', SEN_IDX)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small dataset (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, = data.TabularDataset.splits(\n",
    "        path='/home/wdv2/ats/data/', train='wiki_queries12_head.csv',format='csv', skip_header = True,\n",
    "        fields=[('query_num', QUERY), ('title', TEXT), ('raw_query', TEXT), ('sum', TEXT),\n",
    "                ('story', TEXT), ('sen_vec', SEN_VEC), ('sen_idx', SEN_IDX)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY_NUM:   1\n",
      "TITLE:       ['speed.txt']\n",
      "RAW_QUERY:   ['[', \"'\", '[', '1', ']', 'the', 'average', 'speed', 'of', 'an', 'object', 'in', 'an', 'interval', 'of', 'time', 'is', 'the', 'distance', 'travelled', 'by', 'the', 'object', 'divided', 'by', 'the', 'duration', 'of', 'the', 'interval', ';', '[', '2', ']', 'the', 'instantaneous', 'speed', 'is', 'the', 'limit', 'of', 'the', 'average', 'speed', 'as', 'the', 'duration', 'of', 'the', 'time', 'interval', 'approaches', 'zero', \"'\", \"'\", 'the', 'si', 'unit', 'of', 'speed', 'is', 'the', 'metre', 'per', 'second', 'but', 'the', 'most', 'common', 'unit', 'of', 'speed', 'in', 'everyday', 'usage', 'is', 'the', 'kilometre', 'per', 'hour', 'or', 'in', 'the', 'us', 'and', 'the', 'uk', 'miles', 'per', 'hour', \"'\", \"'\", 'in', 'everyday', 'use', 'and', 'in', 'kinematics', 'the', 'speed', 'of', 'an', 'object', 'is', 'the', 'magnitude', 'of', 'its', 'velocity', '(', 'the', 'rate', 'of', 'change', 'of', 'its', 'position', ')', ';', 'it', 'is', 'thus', 'a', 'scalar', 'quantity', \"'\", \"'\", 'in', 'relativity', 'physics', 'the', 'concept', 'of', 'rapidity', 'replaces', 'the', 'classical', 'idea', 'of', 'speed', \"'\", \"'\", 'the', 'speed', 'equals', 'the', 'time', 'derivative', 'of', \"'\", ']', '<sos>']\n",
      "SUMMARY:     ['in', 'everyday', 'use', 'and', 'in', 'kinematics', 'the', 'speed', 'of', 'an', 'object', 'is', 'the', 'magnitude', 'of', 'its', 'velocity', '(', 'the', 'rate', 'of', 'change', 'of', 'its', 'position', ')', ';', 'it', 'is', 'thus', 'a', 'scalar', 'quantity', '[', '1', ']', 'the', 'average', 'speed', 'of', 'an', 'object', 'in', 'an', 'interval', 'of', 'time', 'is', 'the', 'distance', 'travelled', 'by', 'the', 'object', 'divided', 'by', 'the', 'duration', 'of', 'the', 'interval', ';', '[', '2', ']', 'the', 'instantaneous', 'speed', 'is', 'the', 'limit', 'of', 'the', 'average', 'speed', 'as', 'the', 'duration', 'of', 'the', 'time', 'interval', 'approaches', 'zero', 'speed', 'has', 'the', 'dimensions', 'of', 'distance', 'divided', 'by', 'time', 'the', 'si', 'unit', 'of', 'speed', 'is', 'the', 'metre', 'per', 'second', 'but', 'the', 'most', 'common', 'unit', 'of', 'speed', 'in', 'everyday', 'usage', 'is', 'the', 'kilometre', 'per', 'hour', 'or', 'in', 'the', 'us', 'and', 'the', 'uk', 'miles', 'per', 'hour', 'for', 'air', 'and', 'marine', 'travel', 'the', 'knot', 'is', 'commonly', 'used', 'the', 'fastest', 'possible', 'speed', 'at', 'which', 'energy', 'or', 'information', 'can', 'travel', 'according', 'to', 'special', 'relativity', 'is', 'the', 'speed', 'of', 'light', 'in', 'a', 'vacuum', 'c', '=', '7008299792458000000♠299792458', 'metres', 'per', 'second', '(', 'approximately', '7008299722222222222♠1079000000', 'km/h', 'or', '7008299963840000000♠671000000', 'mph', ')', 'matter', 'cannot', 'quite', 'reach', 'the', 'speed', 'of', 'light', 'as', 'this', 'would', 'require', 'an', 'infinite', 'amount', 'of', 'energy', 'in', 'relativity', 'physics', 'the', 'concept', 'of', 'rapidity', 'replaces', 'the', 'classical', 'idea', 'of', 'speed', '<sos>']\n",
      "STORY_LEN:   3750\n",
      "STORY_HEAD:  ['<sos>', 'this', 'article', 'is', 'about', 'the', 'property', 'of', 'moving', 'bodies', '<sos>', 'for', 'other', 'uses', 'see', 'speed', '(', 'disambiguation', ')', '\"', 'slow', '\"', 'and', '\"', 'slowness', '\"', 'redirect', 'here', '<sos>', 'for', 'other', 'uses', 'see', 'slow', '(', 'disambiguation', ')', 'and', 'slowness', '(', 'disambiguation', ')', '<sos>', 'this', 'article', 'needs', 'additional', 'citations', 'for', 'verification', '<sos>', 'please', 'help', 'improve', 'this', 'article', 'by', 'adding', 'citations', 'to', 'reliable', 'sources', '<sos>', 'unsourced', 'material', 'may', 'be', 'challenged', 'and', 'removed', '<sos>', '(', 'july', '2016', ')', '(', 'learn', 'how', 'and', 'when', 'to', 'remove', 'this', 'template', 'message', ')', '<sos>', 'speed', '<sos>', 'speed', 'can', 'be', 'thought', 'of', 'as', 'the', 'rate', 'at', 'which', 'an']\n"
     ]
    }
   ],
   "source": [
    "example_q = train.examples[1].raw_query\n",
    "\n",
    "example_sum = train.examples[1].sum\n",
    "example_doc = train.examples[1].story\n",
    "ex = train.examples[1].sen_idx\n",
    "#print(ex)\n",
    "#' '.join(example_doc)\n",
    "#print(len(example_q), len(example_sum), len(example_doc))\n",
    "#ex\n",
    "\n",
    "print ('QUERY_NUM:  ', train.examples[1].query_num)\n",
    "print ('TITLE:      ', train.examples[1].title)\n",
    "print ('RAW_QUERY:  ', train.examples[1].raw_query)\n",
    "print ('SUMMARY:    ', train.examples[1].sum)\n",
    "print ('STORY_LEN:  ', len(train.examples[1].story))\n",
    "print ('STORY_HEAD: ', train.examples[1].story[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Vocabulatry\n",
    "We get the vocabulary of the data using 100-dimensional GloVe vectors with the torchtext module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all description &rarr; headline, so if we want to generate text from headline &rarr; description I added the `reverse` flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS token:2\n"
     ]
    }
   ],
   "source": [
    "vocab = TEXT.vocab\n",
    "vocab.vectors.shape\n",
    "SOS_token = vocab.stoi['<sos>']\n",
    "# EOS_token = vocab.stoi['<EOS>']\n",
    "#vocab.stoi\n",
    "print('SOS token:{}'.format(SOS_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator_over_batches(dataset, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    dataset_iter = data.BucketIterator(\n",
    "        dataset, batch_size=batch_size, device=-1, \n",
    "        sort_key=lambda x: len(x.story),\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    dataset_iter.create_batches()\n",
    "    return dataset_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder will take a batch of word sequences, a `LongTensor` of size `(max_len x batch_size)`, and output an encoding for each word, a `FloatTensor` of size `(max_len x batch_size x hidden_size)`.\n",
    "\n",
    "The word inputs are fed through an [embedding layer `nn.Embedding`](http://pytorch.org/docs/nn.html#embedding) to create an embedding for each word, with size `seq_len x hidden_size` (as if it was a batch of words). This is resized to `seq_len x 1 x hidden_size` to fit the expected input of the [GRU layer `nn.GRU`](http://pytorch.org/docs/nn.html#gru). The GRU will return both an output sequence of size `seq_len x hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        if type(input_seqs) == list:\n",
    "            print('List: ', len(input_seqs))\n",
    "        else:\n",
    "            print('np array', input_seqs.shape)\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Bahdanau et al. model\n",
    "\n",
    "The attention model in [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) is described as the following series of equations.\n",
    "\n",
    "Each decoder output is conditioned on the previous outputs and some $\\mathbf x$, where $\\mathbf x$ consists of the current hidden state (which takes into account previous outputs) and the attention \"context\", which is calculated below. The function $g$ is a fully-connected layer with a nonlinear activation, which takes as input the values $y_{i-1}$, $s_i$, and $c_i$ concatenated.\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "The current hidden state $s_i$ is calculated by an RNN $f$ with the last hidden state $s_{i-1}$, last decoder output value $y_{i-1}$, and context vector $c_i$.\n",
    "\n",
    "In the code, the RNN will be a `nn.GRU` layer, the hidden state $s_i$ will be called `hidden`, the output $y_i$ called `output`, and context $c_i$ called `context`.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of all encoder outputs, where each weight $a_{ij}$ is the amount of \"attention\" paid to the corresponding encoder output $h_j$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "... where each weight $a_{ij}$ is a normalized (over all steps) attention \"energy\" $e_{ij}$ ...\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "... where each attention energy is calculated with some function $a$ (such as another linear layer) using the last hidden state $s_{i-1}$ and that particular encoder output $h_j$:\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bahdanau et al. model\n",
    "\n",
    "In summary our decoder should consist of four main parts - an embedding layer turning an input word into a vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The decoder's inputs are the last RNN hidden state $s_{i-1}$, last output $y_{i-1}$, and all encoder outputs $h_*$.\n",
    "\n",
    "* embedding layer with inputs $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* attention layer $a$ with inputs $(s_{i-1}, h_j)$ and outputs $e_{ij}$, normalized to create $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* context vector $c_i$ as an attention-weighted average of encoder outputs\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* RNN layer(s) $f$ with inputs $(s_{i-1}, y_{i-1}, c_i)$ and internal hidden state, outputting $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* an output layer $g$ with inputs $(y_{i-1}, s_i, c_i)$, outputting $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "        #print(embedded.shape)\n",
    "\n",
    "        # run GRU with previous input word\n",
    "        output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "\n",
    "        #print(\"Context size: \", context.size())\n",
    "        #print(\"RNN out size: \", output.size())\n",
    "        #print(\"Output cat Context size: \", torch.cat((output, context), 1).shape)\n",
    "        #print(\"Output size: \", self.out(torch.cat((output, context), 1)).shape)\n",
    "        output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        # Return final output, hidden state\n",
    "        return output, hidden\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        if type(hidden) == Variable:\n",
    "                hidden.detach_() # same as creating a new variable.\n",
    "        else:\n",
    "            for h in hidden: h.detach_() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models\n",
    "\n",
    "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have iter\n",
      "\n",
      "1.00000e-03 *\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "          ...             ⋱             ...          \n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "[torch.FloatTensor of size 9x1437]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    2     2     2  ...      2     2     2\n",
       " 1503    57  1703  ...     21    21    72\n",
       "  158    81     4  ...      3     3   105\n",
       "       ...          ⋱          ...       \n",
       "    1     1     1  ...      1     1  2762\n",
       "    1     1     1  ...      1     1  2202\n",
       "    1     1     1  ...      1     1     2\n",
       "[torch.cuda.LongTensor of size 14517x9 (GPU 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch_size = 9\n",
    "b_iter = get_iterator_over_batches(train, small_batch_size)\n",
    "print('have iter')\n",
    "\n",
    "#input_batches, input_lengths, target_batches, target_lengths, sentence_batches, sentence_lengths = random_batch(batch_size)\n",
    "def parse_batch(batch):\n",
    "    input_batches = batch.story\n",
    "    input_length = len(batch.story)\n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    #TODO: check sentence_batches, sentence_lengths\n",
    "    return input_batches, input_length\n",
    "\n",
    "    #print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
    "    #print('sen_vec', sen_vec.size()) # (batch_size x max_num_sen x vec_len)\n",
    "    #print('sen_idx', sen_idx.size()) # (batch_size x max_len)\n",
    "    #print('target_batches', target_batches.size()) # (max_len x batch_size)\n",
    "\n",
    "# get the first batch to check model fcn\n",
    "for batch in b_iter:\n",
    "    input_batches, input_length = parse_batch(batch)\n",
    "    break\n",
    "input_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create models with a small size (a good idea for eyeball inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(7830, 100)\n",
      "  (gru): GRU(100, 100, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "BahdanauAttnDecoderRNN(\n",
      "  (embedding): Embedding(7830, 100)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (gru): GRU(100, 100, num_layers=2, dropout=0.1)\n",
      "  (out): Linear(in_features=100, out_features=7830, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small_hidden_size = 100 #needs to be size of glove vector embeddings?\n",
    "small_n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(len(vocab), small_hidden_size, small_n_layers)\n",
    "decoder_test = BahdanauAttnDecoderRNN(small_hidden_size, len(vocab), small_n_layers)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    decoder_test.cuda()\n",
    "    \n",
    "print(encoder_test)\n",
    "print(decoder_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the encoder, run the input batch through to get per-batch encoder outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np array torch.Size([14517, 9])\n",
      "encoder_outputs torch.Size([14517, 9, 100])\n",
      "encoder_hidden torch.Size([4, 9, 100])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths, None)\n",
    "\n",
    "print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "print('encoder_hidden', encoder_hidden.size()) # n_layers * 2 x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then starting with a SOS token, run word tokens through the decoder to get each next word token. Instead of doing this with the whole sequence, it is done one at a time, to support using it's own predictions to make the next prediction. This will be one time step at a time, but batched per time step. In order to get this to work for short padded sequences, the batch size is going to get smaller each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input torch.Size([9])\n",
      "sen_vec torch.Size([1437, 9, 100])\n",
      "sen_idx torch.Size([14517, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wdv2/cxz_dev_env/local/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([419, 9, 7830])\n",
      "torch.Size([419, 9])\n",
      "loss 8.97628402709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wdv2/ats/seq2seq_first_bite/masked_cross_entropy.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_flat = functional.log_softmax(logits_flat)\n"
     ]
    }
   ],
   "source": [
    "max_target_length = target_lengths\n",
    "\n",
    "# Prepare decoder input and outputs\n",
    "decoder_input = Variable(torch.LongTensor([vocab.stoi['<sos>']] * small_batch_size))\n",
    "print('decoder_input', decoder_input.size())\n",
    "decoder_hidden = encoder_hidden[:decoder_test.n_layers] # Use last (forward) hidden state from encoder\n",
    "all_decoder_outputs = Variable(torch.zeros(max_target_length, small_batch_size, decoder_test.output_size))\n",
    "\n",
    "print ('sen_vec', sen_vec.shape)\n",
    "print ('sen_idx', sen_idx.shape)\n",
    "\n",
    "if USE_CUDA:\n",
    "    all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    decoder_input = decoder_input.cuda()\n",
    "\n",
    "# Run through decoder one time step at a time\n",
    "for t in range(target_lengths):\n",
    "    decoder_output, decoder_hidden = decoder_test(\n",
    "        decoder_input.contiguous(), decoder_hidden.contiguous(), encoder_outputs.contiguous()\n",
    "    )\n",
    "    all_decoder_outputs[t] = decoder_output # Store this step's outputs\n",
    "    decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    \n",
    "print(all_decoder_outputs.size())\n",
    "print(target_batches.size())\n",
    "\n",
    "# Test masked cross entropy loss\n",
    "loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    target_batches.transpose(0, 1).contiguous(),\n",
    "    torch.LongTensor([vocab.stoi['<sos>']] * target_batches.size(1)),\n",
    "    use_cuda=USE_CUDA\n",
    ")\n",
    "print('loss', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder.\n",
    "\n",
    "### Teacher Forcing and Scheduled Sampling\n",
    "\n",
    "\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster, but [when the trained network is exploited, it may exhibit instability](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - you could think of it as having learned how to listen to the teacher's instructions, without learning how to venture out on its own.\n",
    "\n",
    "The solution to the teacher-forcing \"problem\" is known as [Scheduled Sampling](https://arxiv.org/abs/1506.03099), which simply alternates between using the target values and predicted values when training. We will randomly choose to use teacher forcing with an if statement while training - sometimes we'll feed use real target as the input (ignoring the decoder's output), sometimes we'll use the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = Variable(torch.LongTensor([SOS_token] * 5))\n",
    "torch.equal(decoder_input.data, torch.LongTensor([SOS_token] * 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_batches, input_length, encoder, decoder, encoder_optimizer,\n",
    "                decoder_optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_length, None)\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([vocab.stoi['<sos>']] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    all_decoder_outputs = Variable(torch.zeros(input_length, batch_size, decoder.output_size))\n",
    "    print('target/input_length = ' + str(input_length))\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(input_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs,\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = input_batches[t] # Next input is current target\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        input_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        input_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "\n",
    "To start, we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 100\n",
    "#batch_size = 50\n",
    "dropout= .1\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(len(vocab), hidden_size, n_layers, dropout=dropout)\n",
    "decoder = BahdanauAttnDecoderRNN(hidden_size, len(vocab), n_layers)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     decoder = nn.DataParallel(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 5000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 2\n",
    "evaluate_every = 100\n",
    "\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the progress while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 2 # Reset every print_every\n",
    "plot_loss_total = 2 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    evaluate_and_show_attention(pair['story'], pair['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# import torchvision\n",
    "# from PIL import Image\n",
    "# import visdom\n",
    "# vis = visdom.Visdom()\n",
    "\n",
    "# def show_plot_visdom():\n",
    "#     buf = io.BytesIO()\n",
    "#     plt.savefig(buf)\n",
    "#     buf.seek(0)\n",
    "#     attn_win = 'attention (%s)' % hostname\n",
    "#     vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    #show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "#     output_words, attentions = evaluate(input_sentence)\n",
    "#     output_sentence = ' '.join(output_words)\n",
    "#     print('>', input_sentence)\n",
    "#     if target_sentence is not None:\n",
    "#         print('=', target_sentence)\n",
    "#     print('<', output_sentence)\n",
    "    \n",
    "#     show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "#     # Show input, target, output text in visdom\n",
    "#     win = 'evaluted (%s)' % hostname\n",
    "#     text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "#     vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "**TODO** Run `train_epochs` for `n_epochs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the train function many times, printing a summary as we go.\n",
    "\n",
    "*Note:* If you're running this notebook you can **train, interrupt, evaluate, and come back to continue training**. Simply run the notebook starting from the following cell (running from the previous cell will reset the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-03 *\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "          ...             ⋱             ...          \n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "-1.3350 -1.3350 -1.3350  ...  -1.3350 -1.3350 -1.3350\n",
      "[torch.FloatTensor of size 9x1437]\n",
      "\n",
      "np array torch.Size([14517, 9])\n",
      "target/input_lengths = 14517\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1b1247ef845b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-079a4e21112d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(input_batches, input_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mall_decoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_decoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Run through decoder one time step at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device, async)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cxz_dev_env/local/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    b_iter = get_iterator_over_batches(train, small_batch_size)\n",
    "    for batch in b_iter:\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        input_batches, input_length = parse_batch(batch)\n",
    "\n",
    "        #def train(input_batches, input_lengths, target_batches, target_lengths, sentence_batches, sentence_lengths,\n",
    "        #      encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "        # Run the train function\n",
    "        loss, ec, dc = train_model(\n",
    "            input_batches, input_length,\n",
    "            encoder, decoder,\n",
    "            encoder_optimizer, decoder_optimizer, criterion\n",
    "        )\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        eca += ec\n",
    "        dca += dc\n",
    "\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "        ##job.record(epoch, loss)\n",
    "\n",
    "    #     if epoch % 1 == 0:\n",
    "    #         print_loss_avg = print_loss_total / print_every\n",
    "    #         print_loss_total = 0\n",
    "    #         print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "    #         print(print_summary)\n",
    "\n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        \n",
    "        '''\n",
    "        ecs_win = 'encoder grad (%s)' % hostname\n",
    "        dcs_win = 'decoder grad (%s)' % hostname\n",
    "        vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "        vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "        '''\n",
    "\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\"premier romano prodi battled tuesday for any votes freed up from a split in a far left party but said he will resign if he loses a confidence vote expected later this week .\")\n",
    "plt.matshow(attentions.numpy())\n",
    "show_plot_visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"a south korean lawmaker said friday communist north korea could be producing plutonium and could have more secret underground nuclear facilities than already feared .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"egyptian president hosni mubarak met here sunday with syrian president hafez assad to try to defuse growing tension between syria and turkey .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"police and soldiers on friday blocked off the street in front of a house where members of a terrorist gang are believed to have assembled the bomb that blew up the u .s . embassy killing people .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"premier romano prodi battled tuesday for any votes freed up from a split in a far left party but said he will resign if he loses a confidence vote expected later this week .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "\n",
    "* Try with a different dataset\n",
    "    * cnn/dailymail\n",
    "    * gigawords\n",
    "    * standford\n",
    "    * Human &rarr; Machine (e.g. IOT commands)\n",
    "    * Chat &rarr; Response\n",
    "    * Question &rarr; Answer\n",
    "* Replace the embedding pre-trained word embeddings such as word2vec or GloVe\n",
    "* Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
    "* Try different RNN layers like lstm.\n",
    "* Add batch operation for GPU training\n",
    "* Add beam search on decoder side when dealing with long documents.\n",
    "* Control the Different output size\n",
    "* Dig out other tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
