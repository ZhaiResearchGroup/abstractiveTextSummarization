{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Summarization with a Sequence to Sequence Network and Attention on Language Model\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a single vector, and a decoder network unfolds that vector into a new sequence. \n",
    "\n",
    "To improve upon this model we'll use an [attention mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder learn to focus over a specific range of the input sequence. This is initially designed for data-driven language translation, in which case the length of input and output sequence is approximately similar. For text summarization task, in which case the output sequence is much shorter then the input, the model didn't perform that well. Some other tricks should be explored soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequence to Sequence model\n",
    "\n",
    "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **encoder** and **decoder**. The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. The decoder uses this context vector to produce a sequence of outputs one step at a time.\n",
    "\n",
    "![](https://i.imgur.com/tVtHhNp.png)\n",
    "\n",
    "When using a single RNN, there is a one-to-one relationship between inputs and outputs. We would quickly run into problems with different sequence orders and lengths that are common during translation. Consider the simple sentence \"Je ne suis pas le chat noir\" &rarr; \"I am not the black cat\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n",
    "\n",
    "With the seq2seq model, by encoding many inputs into one vector, and decoding from one vector into many outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector, a single point in some N dimensional space of sequences. In an ideal case, this point can be considered the \"meaning\" of the sequence.\n",
    "\n",
    "This idea can be extended beyond sequences. Image captioning tasks take an [image as input, and output a description](https://arxiv.org/abs/1411.4555) of the image (img2seq). Some image generation tasks take a [description as input and output a generated image](https://arxiv.org/abs/1511.02793) (seq2img). These models can be referred to more generally as \"encoder decoder\" networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Mechanism\n",
    "\n",
    "The fixed-length vector carries the burden of encoding the the entire \"meaning\" of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. Imagine two nearly identical sentences, twenty words long, with only one word different. Both the encoders and decoders must be nuanced enough to represent that change as a very slightly different point in space.\n",
    "\n",
    "The **attention mechanism** [introduced by Bahdanau et al.](https://arxiv.org/abs/1409.0473) addresses this by giving the decoder a way to \"pay attention\" to parts of the input, rather than relying on a single vector. For every step the decoder can select a different part of the input sentence to consider.\n",
    "\n",
    "![](https://i.imgur.com/5y6SCvU.png)\n",
    "\n",
    "Attention is calculated with another feedforward layer in the decoder. This layer will use the current input and hidden state to create a new vector, which is the same size as the input sequence (in practice, a fixed maximum length). This vector is processed through softmax to create *attention weights*, which are multiplied by the encoders' outputs to create a new context vector, which is then used to predict the next output.\n",
    "\n",
    "![](https://i.imgur.com/K1qMPxs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "'''\n",
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
    "from masked_cross_entropy import *\n",
    "torch.version.cuda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. **If you don't have a GPU, set this to `False`**. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data files\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs.\n",
    "\n",
    "[This question on Open Data Stack Exchange](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages) pointed me to the open translation site http://tatoeba.org/ which has downloads available at http://tatoeba.org/eng/downloads - and better yet, someone did the extra work of splitting language pairs into individual text files here: http://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so download `fra-eng.zip`, extract the text file in there, and rename it to `data/eng-fra.txt` before continuing (for some reason the zipfile is named backwards). The file is a tab separated list of translation pairs:\n",
    "\n",
    "```\n",
    "I am cold.    Je suis froid.\n",
    "```\n",
    "\n",
    "For this summarization experiment, we adopt **DUC2003 dataset**, which is stored as the `/data/desc.txt` and `/data/head.txt` for input and target file. All the lines in these two files are seprerated by `\\n` and are in the same order between themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.0.61'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language. On other tutorials,  they adopt an initial embedding matrix built from [GloVe](http://nlp.stanford.edu/projects/glove/), which can be tested afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Sentence Embeddings\n",
    "We'll use the gensim's library of pretrained sentence-to-vector models originally trained on the AP News dataset to use as a sentence-level analogue to semantic word embeddings. This is used as the second layer of the hierarchical attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "sentence_size = 100\n",
    "\n",
    "sent2vec = gensim.models.doc2vec.Doc2Vec.load(\"../model/apnews_sen_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [ 0.00341425 -0.04075036  0.35105264  0.05755128 -0.17433901  0.07325942\n",
      "  0.22250323 -0.01097782 -0.35341799 -0.1036211   0.12697797  0.04047842\n",
      "  0.11696927 -0.28646004 -0.25921738 -0.00562044 -0.11523465 -0.22976226\n",
      " -0.11566306 -0.04553635  0.02986817  0.05691364 -0.1618664   0.18014768\n",
      "  0.18349031 -0.11264053 -0.09587328 -0.11210064  0.05579985  0.25203061\n",
      " -0.02596972 -0.07003354  0.06985041 -0.26066655 -0.09205133  0.0460217\n",
      " -0.16363902  0.25614142 -0.25599369 -0.29436219 -0.16214657  0.01524232\n",
      "  0.17072764  0.19897856  0.10745639 -0.02563397  0.05738298  0.13599092\n",
      "  0.1884694   0.11272503 -0.07851743 -0.02930761  0.12749232  0.08712669\n",
      " -0.18818183 -0.00655917 -0.07853978 -0.38990811 -0.11784644  0.02738558\n",
      " -0.04836019  0.0895314  -0.1280348  -0.09844898 -0.1054436   0.01445332\n",
      "  0.21938069 -0.30845314  0.01052343 -0.12965555  0.08516671  0.02122386\n",
      "  0.01343225  0.14131147 -0.01593197  0.21185912 -0.08159496  0.00063765\n",
      "  0.17674914  0.0499947   0.02518339  0.12858617  0.07488132  0.42694151\n",
      "  0.06564853 -0.01504276 -0.07598649  0.02057783 -0.072868   -0.10880776\n",
      "  0.17296873  0.06563497 -0.02044961  0.06777309  0.11427342  0.08101634\n",
      "  0.1363302  -0.08764303  0.02822297 -0.02872043]\n"
     ]
    }
   ],
   "source": [
    "embedding = sent2vec.infer_vector('hark the herald angels sing')\n",
    "print(len(embedding), embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.LongTensor(embedding)\n",
    "cten = tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torchtext.vocab as vocab\n",
    "from torchtext.data import RawField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Query Index</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Raw Query</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Sen_Vecs</th>\n",
       "      <th>Sen_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>James Bond.txt</td>\n",
       "      <td>['As of 2018, there have been twenty-four film...</td>\n",
       "      <td>The James Bond series focuses on a fictional B...</td>\n",
       "      <td>This article is about the spy series in genera...</td>\n",
       "      <td>This article is about the spy series in genera...</td>\n",
       "      <td>This article is about the spy series in genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Speed.txt</td>\n",
       "      <td>['[1] The average speed of an object in an int...</td>\n",
       "      <td>In everyday use and in kinematics, the speed o...</td>\n",
       "      <td>This article is about the property of moving b...</td>\n",
       "      <td>This article is about the property of moving b...</td>\n",
       "      <td>This article is about the property of moving b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Official language.txt</td>\n",
       "      <td>['[1] Since \"the means of expression of a peop...</td>\n",
       "      <td>An official language is a language that is giv...</td>\n",
       "      <td>An official language is a language that is giv...</td>\n",
       "      <td>An official language is a language that is giv...</td>\n",
       "      <td>An official language is a language that is giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Federal Register.txt</td>\n",
       "      <td>['The Federal Register is compiled by the Offi...</td>\n",
       "      <td>The Federal Register (FR or sometimes Fed. Reg...</td>\n",
       "      <td>Federal Register. Cover. Type. Daily official ...</td>\n",
       "      <td>Federal Register. Cover. Type. Daily official ...</td>\n",
       "      <td>Federal Register. Cover. Type. Daily official ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Flash memory.txt</td>\n",
       "      <td>['Although flash memory is technically a type ...</td>\n",
       "      <td>Flash memory is an electronic (solid-state) no...</td>\n",
       "      <td>For the neuropsychological concept related to ...</td>\n",
       "      <td>For the neuropsychological concept related to ...</td>\n",
       "      <td>For the neuropsychological concept related to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Query Index                 Titles  \\\n",
       "0           0            0         James Bond.txt   \n",
       "1           1            1              Speed.txt   \n",
       "2           2            2  Official language.txt   \n",
       "3           3            3   Federal Register.txt   \n",
       "4           4            4       Flash memory.txt   \n",
       "\n",
       "                                           Raw Query  \\\n",
       "0  ['As of 2018, there have been twenty-four film...   \n",
       "1  ['[1] The average speed of an object in an int...   \n",
       "2  ['[1] Since \"the means of expression of a peop...   \n",
       "3  ['The Federal Register is compiled by the Offi...   \n",
       "4  ['Although flash memory is technically a type ...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  The James Bond series focuses on a fictional B...   \n",
       "1  In everyday use and in kinematics, the speed o...   \n",
       "2  An official language is a language that is giv...   \n",
       "3  The Federal Register (FR or sometimes Fed. Reg...   \n",
       "4  Flash memory is an electronic (solid-state) no...   \n",
       "\n",
       "                                           Documents  \\\n",
       "0  This article is about the spy series in genera...   \n",
       "1  This article is about the property of moving b...   \n",
       "2  An official language is a language that is giv...   \n",
       "3  Federal Register. Cover. Type. Daily official ...   \n",
       "4  For the neuropsychological concept related to ...   \n",
       "\n",
       "                                            Sen_Vecs  \\\n",
       "0  This article is about the spy series in genera...   \n",
       "1  This article is about the property of moving b...   \n",
       "2  An official language is a language that is giv...   \n",
       "3  Federal Register. Cover. Type. Daily official ...   \n",
       "4  For the neuropsychological concept related to ...   \n",
       "\n",
       "                                            Sen_idxs  \n",
       "0  This article is about the spy series in genera...  \n",
       "1  This article is about the property of moving b...  \n",
       "2  An official language is a language that is giv...  \n",
       "3  Federal Register. Cover. Type. Daily official ...  \n",
       "4  For the neuropsychological concept related to ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# note. this is the wrong dataset right now. (I think)\n",
    "data_df = pd.read_csv('../data/wiki_short.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.FloatTensor(embedding)\n",
    "tensor = tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0034\n",
       "-0.0408\n",
       " 0.3511\n",
       " 0.0576\n",
       "-0.1743\n",
       "[torch.cuda.FloatTensor of size 5 (GPU 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "vectors = sparse.load_npz('../data/queries_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(batch):#, device, train):\n",
    "    \"\"\" Process a list of examples to create a torch.Tensor.\n",
    "    Pad, numericalize, and postprocess a batch and create a tensor.\n",
    "    Args:\n",
    "        batch (list(object)): A list of object from a batch of examples.\n",
    "    Returns:\n",
    "        torch.autograd.Variable: Processed object given the input\n",
    "            and custom postprocessing Pipeline.\n",
    "    \"\"\"\n",
    "    # vectors_file = sparse.load_npz('../data/queries_matrix.npz')\n",
    "\n",
    "    csr_vec = vectors[batch[0]]\n",
    "    vec = sparse.csr_matrix.todense(csr_vec)\n",
    "    tensor = torch.LongTensor(vec)\n",
    "    \n",
    "    ret_matrix = tensor\n",
    "    \n",
    "    for q_idx in batch[1::]:\n",
    "        #print(q_idx)\n",
    "        csr_vec = vectors[batch[0]]\n",
    "        vec = sparse.csr_matrix.todense(csr_vec)\n",
    "        tensor = torch.LongTensor(vec)\n",
    "        ret_matrix = torch.cat((ret_matrix, tensor),1)\n",
    "    \n",
    "    #if device != -1:\n",
    "    ret_matrix = ret_matrix.contiguous().cuda()\n",
    "    \n",
    "    return torch.autograd.Variable(ret_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_vec_postprocess(batch):\n",
    "    \n",
    "    tokenized_batch = [nltk.sent_tokenize(example) for example in batch]\n",
    "    max_sen_len = np.max(np.array([len(ex) for ex in tokenized_batch]))\n",
    "    \n",
    "    batch_vec = np.ndarray((0,max_sen_len, 100))\n",
    "    for example in tokenized_batch:\n",
    "        sentence_vectors = []\n",
    "        example_sen_len = len(example) # lenth of this example tells us how much we need to pad\n",
    "        for i in range(max_sen_len):\n",
    "            if i < example_sen_len:\n",
    "                sentence_vectors.append(np.array(sent2vec.infer_vector(example[i])))\n",
    "            else:\n",
    "                sentence_vectors.append(np.zeros(100))\n",
    "        sv = np.array(sentence_vectors)\n",
    "        print(sv.shape)\n",
    "        batch_vec = np.concatenate( (batch_vec, sv.reshape(1,max_sen_len,100)), axis=0 )\n",
    "        print(batch_vec.shape)\n",
    "    tensor = torch.LongTensor(batch_vec)\n",
    "    tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, init_token='<SOS>', eos_token='<EOS>',lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY = data.RawField(postprocessing=postprocess) # sequential=False, use_vocab = False, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEN_VEC = data.RawField(postprocessing=sen_vec_postprocess) #, sequential=False, use_vocab = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524288000"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "# need to do this so we dont get errors about having too big of a file in a single cell of a csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, = data.TabularDataset.splits(\n",
    "        path='../data/', train='wiki_short.csv',format='csv', skip_header = True,\n",
    "        fields=[('query_num', QUERY), ('title', TEXT), ('raw_query', TEXT),\n",
    "                ('sum', TEXT), ('story', TEXT), ('sen_vec', SEN_VEC), ('sen_idx', SEN_VEC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about the property of moving bodies. For other uses, see Speed (disambiguation).. \"Slow\" and \"Slowness\" redirect here. For other uses, see Slow (disambiguation) and Slowness (disambiguation).. This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2016) (Learn how and when to remove this template message). Speed. Speed can be thought of as the rate at which an object covers distance. A fast-moving object has a high speed and covers a relatively large distance in a given amount of time, while a slow-moving object covers a relatively small amount of distance in the same amount of time.. Common symbols. v. SI¬†unit. m/s, m¬†s‚àí1. Classical mechanics. F. ‚Üí. =. m. a. ‚Üí. {\\displaystyle {\\vec {F}}=m{\\vec {a}}}. Second law of motion. History. Timeline. Branches. Applied. Celestial. Continuum. Dynamics. Kinematics. Kinetics. Statics. Statistical. Fundamentals. Acceleration. Angular momentum. Couple. D'Alembert's principle. Energy. kinetic. potential. Force. Frame of reference. Impulse. Inertia¬†/ Moment of inertia. Mass. Mechanical power. Mechanical work. Moment. Momentum. Space. Speed. Time. Torque. Velocity. Virtual work. Formulations. Newton's laws of motion. Analytical mechanics. Lagrangian mechanics. Hamiltonian mechanics. Routhian mechanics. Hamilton‚ÄìJacobi equation. Appell's equation of motion. Udwadia‚ÄìKalaba equation. Koopman‚Äìvon Neumann mechanics. Core topics. Damping¬†(ratio). Displacement. Equations of motion. Euler's laws of motion. Fictitious force. Friction. Harmonic oscillator. Inertial¬†/ Non-inertial reference frame. Mechanics of planar particle motion. Motion¬†(linear). Newton's law of universal gravitation. Newton's laws of motion. Relative velocity. Rigid body. dynamics. Euler's equations. Simple harmonic motion. Vibration. Rotation. Circular motion. Rotating reference frame. Centripetal force. Centrifugal force. reactive. Coriolis force. Pendulum. Tangential speed. Rotational speed. Angular acceleration¬†/ displacement¬†/ frequency¬†/ velocity. Scientists. Galileo. Newton. Kepler. Horrocks. Halley. Euler. d'Alembert. Clairaut. Lagrange. Laplace. Hamilton. Poisson. Daniel Bernoulli. Johann Bernoulli. Cauchy. v. t. e. In everyday use and in kinematics, the speed of an object is the magnitude of its velocity (the rate of change of its position); it is thus a scalar quantity.[1] The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval;[2] the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.. Speed has the dimensions of distance divided by time. The SI unit of speed is the metre per second, but the most common unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.. The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum c = 7008299792458000000‚ô†299792458 metres per second (approximately 7008299722222222222‚ô†1079000000¬†km/h or 7008299963840000000‚ô†671000000¬†mph). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed.. Contents. 1 Definition. 1.1 Historical definition. 1.2 Instantaneous speed. 1.3 Average speed. 1.4 Tangential speed. 2 Units. 3 Examples of different speeds. 4 Psychology. 5 See also. 6 References. Definition[edit]. Historical definition[edit]. Italian physicist Galileo Galilei is usually credited with being the first to measure speed by considering the distance covered and the time it takes. Galileo defined speed as the distance covered per unit of time.[3] In equation form, this is. v. =. d. t. ,. {\\displaystyle v={\\frac {d}{t}},}. where. v. {\\displaystyle v}. is speed,. d. {\\displaystyle d}. is distance, and. t. {\\displaystyle t}. is time. A cyclist who covers 30 metres in a time of 2 seconds, for example, has a speed of 15 metres per second. Objects in motion often have variations in speed (a car might travel along a street at 50¬†km/h, slow to 0¬†km/h, and then reach 30¬†km/h).. Instantaneous speed[edit]. Speed at some instant, or assumed constant during a very short period of time, is called instantaneous speed. By looking at a speedometer, one can read the instantaneous speed of a car at any instant.[3] A car travelling at 50¬†km/h generally goes for less than one hour at a constant speed, but if it did go at that speed for a full hour, it would travel 50¬†km. If the vehicle continued at that speed for half an hour, it would cover half that distance (25¬†km). If it continued for only one minute, it would cover about 833¬†m.. In mathematical terms, the instantaneous speed. v. {\\displaystyle v}. is defined as the magnitude of the instantaneous velocity. v. {\\displaystyle {\\boldsymbol {v}}}. , that is, the derivative of the position. r. {\\displaystyle {\\boldsymbol {r}}}. with respect to time:[2][4]. v. =. |. v. |. =. |. r. Àô. |. =. |. d. r. d. t. |. .. {\\displaystyle v=\\left|{\\boldsymbol {v}}\\right|=\\left|{\\dot {\\boldsymbol {r}}}\\right|=\\left|{\\frac {d{\\boldsymbol {r}}}{dt}}\\right|\\,.}. If. s. {\\displaystyle s}. is the length of the path (also known as the distance) travelled until time. t. {\\displaystyle t}. , the speed equals the time derivative of. s. {\\displaystyle s}. :[2]. v. =. d. s. d. t. .. {\\displaystyle v={\\frac {ds}{dt}}.}. In the special case where the velocity is constant (that is, constant speed in a straight line), this can be simplified to. v. =. s. /. t. {\\displaystyle v=s/t}. . The average speed over a finite time interval is the total distance travelled divided by the time duration.. Average speed[edit]. Different from instantaneous speed, average speed is defined as the total distance covered divided by the time interval. For example, if a distance of 80 kilometres is driven in 1 hour, the average speed is 80 kilometres per hour. Likewise, if 320 kilometres are travelled in 4 hours, the average speed is also 80 kilometres per hour. When a distance in kilometres (km) is divided by a time in hours (h), the result is in kilometres per hour (km/h). Average speed does not describe the speed variations that may have taken place during shorter time intervals (as it is the entire distance covered divided by the total time of travel), and so average speed is often quite different from a value of instantaneous speed.[3] If the average speed and the time of travel are known, the distance travelled can be calculated by rearranging the definition to. d. =. v. ¬Ø. t. .. {\\displaystyle d={\\boldsymbol {\\bar {v}}}t\\,.}. Using this equation for an average speed of 80 kilometres per hour on a 4-hour trip, the distance covered is found to be 320 kilometres.. Expressed in graphical language, the slope of a tangent line at any point of a distance-time graph is the instantaneous speed at this point, while the slope of a chord line of the same graph is the average speed during the time interval covered by the chord.. Tangential speed[edit]. Linear speed is the distance travelled per unit of time, while tangential speed (or tangential velocity) is the linear speed of something moving along a circular path.[5] A point on the outside edge of a merry-go-round or turntable travels a greater distance in one complete rotation than a point nearer the center. Travelling a greater distance in the same time means a greater speed, and so linear speed is greater on the outer edge of a rotating object than it is closer to the axis. This speed along a circular path is known as tangential speed because the direction of motion is tangent to the circumference of the circle. For circular motion, the terms linear speed and tangential speed are used interchangeably, and both use units of m/s, km/h, and others.. Rotational speed (or angular speed) involves the number of revolutions per unit of time. All parts of a rigid merry-go-round or turntable turn about the axis of rotation in the same amount of time. Thus, all parts share the same rate of rotation, or the same number of rotations or revolutions per unit of time. It is common to express rotational rates in revolutions per minute (RPM) or in terms of the number of \"radians\" turned in a unit of time. There are little more than 6 radians in a full rotation (2œÄ radians exactly). When a direction is assigned to rotational speed, it is known as rotational velocity or angular velocity. Rotational velocity is a vector whose magnitude is the rotational speed.. Tangential speed and rotational speed are related: the greater the RPMs, the larger the speed in metres per second. Tangential speed is directly proportional to rotational speed at any fixed distance from the axis of rotation.[5] However, tangential speed, unlike rotational speed, depends on radial distance (the distance from the axis). For a platform rotating with a fixed rotational speed, the tangential speed in the centre is zero. Towards the edge of the platform the tangential speed increases proportional to the distance from the axis.[6] In equation form:. v. ‚àù. r. œâ. ,. {\\displaystyle v\\propto \\!\\,r\\omega \\,,}. where v is tangential speed and œâ (Greek letter omega) is rotational speed. One moves faster if the rate of rotation increases (a larger value for œâ), and one also moves faster if movement farther from the axis occurs (a larger value for r). Move twice as far from the rotational axis at the centre and you move twice as fast. Move out three times as far and you have three times as much tangential speed. In any kind of rotating system, tangential speed depends on how far you are from the axis of rotation.. When proper units are used for tangential speed v, rotational speed œâ, and radial distance r, the direct proportion of v to both r and œâ becomes the exact equation. v. =. r. œâ. .. {\\displaystyle v=r\\omega \\,.}. Thus, tangential speed will be directly proportional to r when all parts of a system simultaneously have the same œâ, as for a wheel, disk, or rigid wand.. Units[edit]. Main article: Conversion of units ¬ß¬†Speed or velocity. Units of speed include:. metres per second (symbol m¬†s‚àí1 or m/s), the SI derived unit;. kilometres per hour (symbol km/h);. miles per hour (symbol mi/h or mph);. knots (nautical miles per hour, symbol kn or kt);. feet per second (symbol fps or ft/s);. Mach number (dimensionless), speed divided by the speed of sound;. in natural units (dimensionless), speed divided by the speed of light in vacuum (symbol c = 7008299792458000000‚ô†299792458¬†m/s).. Conversions between common units of speed. m/s. km/h. mph. knot. ft/s. 1 m/s =. 1. 3.6. 7000223693600000000‚ô†2.236936. 7000194384400000000‚ô†1.943844. 7000328084000000000‚ô†3.280840. 1 km/h =. 6999277778000000000‚ô†0.277778. 1. 6999621371000000000‚ô†0.621371. 6999539957000000000‚ô†0.539957. 6999911344000000000‚ô†0.911344. 1 mph =. 6999447040000000000‚ô†0.44704. 7000160934400000000‚ô†1.609344. 1. 6999868976000000000‚ô†0.868976. 7000146666700000000‚ô†1.466667. 1 knot =. 6999514444000000000‚ô†0.514444. 1.852. 7000115077900000000‚ô†1.150779. 1. 7000168781000000000‚ô†1.687810. 1 ft/s =. 6999304800000000000‚ô†0.3048. 7000109728000000000‚ô†1.09728. 6999681818000000000‚ô†0.681818. 6999592484000000000‚ô†0.592484. 1. (Values in bold face are exact.). Examples of different speeds[edit]. This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message). This section may contain indiscriminate, excessive, or irrelevant examples. Please improve the article by adding more descriptive text and removing less pertinent examples. See Wikipedia's guide to writing better articles for further suggestions. (May 2014). Main article: Orders of magnitude (speed). Speed. m/s. ft/s. km/h. mph. Notes. Approximate rate of continental drift. 6992100000000000000‚ô†0.00000001. 6992300000000000000‚ô†0.00000003. 6992400000000000000‚ô†0.00000004. 6992200000000000000‚ô†0.00000002. 4¬†cm/year. Varies depending on location.. Speed of a common snail. 0.001. 0.003. 0.004. 0.002. 1 millimetre per second. A brisk walk. 1.7. 5.5. 6.1. 3.8. A typical road cyclist. 4.4. 14.4. 16. 10. Varies widely by person, terrain, bicycle, effort, weather. A fast martial arts kick. 7.7. 25.2. 27.7. 17.2. Fastest kick recorded at 130 milliseconds from floor to target at 1 meter distance. Average velocity speed across kick duration[7]. Sprint runners. 12.2. 40. 43.92. 27. Usain Bolt's 100 metres world record.. Approximate average speed of road cyclists. 12.5. 41.0. 45. 28. On flat terrain, will vary. Typical suburban speed limit in most of the world. 13.8. 45.3. 50. 30. Taipei 101 observatory elevator. 16.7. 54.8. 60.6. 37.6. 1010 m/min. Typical rural speed limit. 24.6. 80.66. 88.5. 56. British National Speed Limit (single carriageway). 26.8. 88. 96.56. 60. Category 1 hurricane. 33. 108. 119. 74. Minimum sustained speed over 1 minute. Speed limit on a French autoroute. 36.1. 118. 130. 81. Highest recorded human-powered speed. 37.02. 121.5. 133.2. 82.8. Sam Whittingham in a recumbent bicycle[8]. Muzzle velocity of a paintball marker. 90. 295. 320. 200. Cruising speed of a Boeing 747-8 passenger jet. 255. 836. 917. 570. Mach 0.85 at 7004106680000000000‚ô†35000¬†ft (7004106680000000000‚ô†10668¬†m) altitude. The official land speed record. 341.1. 1119.1. 1227.98. 763. The speed of sound in dry air at sea-level pressure and 20¬†¬∞C. 343. 7003112500000000000‚ô†1125. 7003123500000000000‚ô†1235. 768. Mach 1 by definition. 20¬†¬∞C = 293.15¬†kelvins.. Muzzle velocity of a 7.62x39mm cartridge. 710. 7003233000000000000‚ô†2330. 7003260000000000000‚ô†2600. 7003160000000000000‚ô†1600. The 7.62√ó39mm round is a rifle cartridge of Soviet origin. Official flight airspeed record for jet engined aircraft. 980. 7003321500000000000‚ô†3215. 7003353000000000000‚ô†3530. 7003219400000000000‚ô†2194. Lockheed SR-71 Blackbird. Space shuttle on re-entry. 7003780000000000000‚ô†7800. 7004256000000000000‚ô†25600. 7004280000000000000‚ô†28000. 17,500. Escape velocity on Earth. 7004112000000000000‚ô†11200. 7004367000000000000‚ô†36700. 7004400000000000000‚ô†40000. 7004250000000000000‚ô†25000. 11.2¬†km¬∑s‚àí1. Voyager 1 relative velocity to the Sun in 2013. 7004170000000000000‚ô†17000. 7004558000000000000‚ô†55800. 7004612000000000000‚ô†61200. 7004380000000000000‚ô†38000. Fastest heliocentric recession speed of any humanmade object.[9] (11¬†mi/s). Average orbital speed of planet Earth around the Sun. 7004297830000000000‚ô†29783. 7004977130000000000‚ô†97713. 7005107218000000000‚ô†107218. 7004666230000000000‚ô†66623. The fastest recorded speed of the Helios probes.. 70,220. 230,381. 252,792. 157,078. Recognized as the fastest speed achieved by a man-made spacecraft, achieved in solar orbit.. Speed of light in vacuum (symbol c). 7008299792458000000‚ô†299792458. 7008983571056000000‚ô†983571056. 7009107925284800000‚ô†1079252848. 7008670616629000000‚ô†670616629. Exactly 7008299792458000000‚ô†299792458¬†m/s, by definition of the metre. Psychology[edit]. According to Jean Piaget, the intuition for the notion of speed in humans precedes that of duration, and is based on the notion of outdistancing.[10] Piaget studied this subject inspired by a question asked to him in 1928 by Albert Einstein: \"In what order do children acquire the concepts of time and speed?\"[11] Children's early concept of speed is based on \"overtaking\", taking only temporal and spatial orders into consideration, specifically: \"A moving object is judged to be more rapid than another when at a given moment the first object is behind and a moment or so later ahead of the other object.\"[12]. See also[edit]. Air speed. Land speed. List of vehicle speed records. Typical projectile speeds. Speedometer. V speeds. References[edit]. Look up speed¬†or swiftness in Wiktionary, the free dictionary.. Wikiquote has quotations related to: Speed. Richard P. Feynman, Robert B. Leighton, Matthew Sands. The Feynman Lectures on Physics, Volume I, Section 8-2. Addison-Wesley, Reading, Massachusetts (1963). ISBN¬†0-201-02116-1.. ^ Wilson, Edwin Bidwell (1901). Vector analysis: a text-book for the use of students of mathematics and physics, founded upon the lectures of J. Willard Gibbs. p.¬†125.¬† This is the likely origin of the speed/velocity terminology in vector physics.. ^ a b c Elert, Glenn. \"Speed & Velocity\". The Physics Hypertextbook. Retrieved 8 June 2017.. ^ a b c Hewitt (2006), p. 42. ^ \"IEC 60050 - Details for IEV number 113-01-33: \"speed\"\". Electropedia: The World's Online Electrotechnical Vocabulary. Retrieved 2017-06-08.. ^ a b Hewitt (2006), p. 131. ^ Hewitt (2006), p. 132. ^ http://www.kickspeed.com.au/Improve-measure-kicking-speed.html. ^ http://www.wisil.recumbents.com/wisil/whpsc2009/results.htm. ^ Darling, David. \"Fastest Spacecraft\". Retrieved August 19, 2013.. ^ Jean Piaget, Psychology and Epistemology: Towards a Theory of Knowledge, The Viking Press, pp. 82‚Äì83 and pp. 110‚Äì112, 1973. SBN 670-00362-x. ^ Siegler, Robert S.; Richards, D. Dean (1979). \"Development of Time, Speed, and Distance Concepts\" (PDF). Developmental Psychology. 15 (3): 288‚Äì298. doi:10.1037/0012-1649.15.3.288.. ^ Rod Parker-Rees and Jenny William, eds. (2006). Early Years Education: Histories and Traditions, Volume 1. Taylor & Francis. p.¬†164.¬†CS1 maint: Uses editors parameter (link). v. t. e. Kinematics. ‚Üê Integrate ‚Ä¶ Differentiate ‚Üí. Absement. Displacement (Distance). Velocity (Speed). Acceleration. Jerk. Jounce. v. t. e. Classical mechanics SI units. Linear/translational quantities. Angular/rotational quantities. Dimensions. 1. L. L2. Dimensions. 1. 1. 1. T. time: t. s. absement: A. m s. T. time: t. s. 1. distance: d, position: r, s, x, displacement. m. area: A. m2. 1. angle: Œ∏, angular displacement: Œ∏. rad. solid angle: Œ©. rad2, sr. T‚àí1. frequency: f. s‚àí1, Hz. speed: v, velocity: v. m s‚àí1. kinematic viscosity: ŒΩ,. specific angular momentum:¬†h. m2 s‚àí1. T‚àí1. frequency: f. s‚àí1, Hz. angular speed: œâ, angular velocity: œâ. rad¬†s‚àí1. T‚àí2. acceleration: a. m s‚àí2. T‚àí2. angular acceleration: Œ±. rad¬†s‚àí2. T‚àí3. jerk: j. m s‚àí3. T‚àí3. angular jerk: Œ∂. rad¬†s‚àí3. M. mass: m. kg. ML2. moment of inertia:¬†I. kg¬†m2. MT‚àí1. momentum: p, impulse: J. kg¬†m¬†s‚àí1, N s. action: ùíÆ, actergy: ‚Ñµ. kg¬†m2¬†s‚àí1, J s. ML2T‚àí1. angular momentum: L, angular impulse: ŒîL. kg¬†m2¬†s‚àí1. action: ùíÆ, actergy: ‚Ñµ. kg¬†m2¬†s‚àí1, J¬†s. MT‚àí2. force: F, weight: Fg. kg m s‚àí2, N. energy: E, work: W. kg m2 s‚àí2, J. ML2T‚àí2. torque: œÑ, moment: M. kg m2 s‚àí2, N m. energy: E, work: W. kg m2 s‚àí2, J. MT‚àí3. yank: Y. kg m s‚àí3, N s‚àí1. power: P. kg m2 s‚àí3,¬†W. ML2T‚àí3. rotatum: P. kg m2 s‚àí3, N m s‚àí1. power: P. kg m2 s‚àí3,¬†W. Retrieved from \"https://en.wikipedia.org/w/index.php?title=Speed&oldid=817347057\"\t\t\t\t\tCategories: Physical quantitiesVelocityHidden categories: CS1 maint: Uses editors parameterArticles needing additional references from July 2016All articles needing additional referencesUse British English from September 2015Pages using deprecated image syntaxArticles needing additional references from May 2013Articles with too many examples\n",
      "1 131 193\n"
     ]
    }
   ],
   "source": [
    "example_q = train.examples[1].raw_query\n",
    "\n",
    "example_sum = train.examples[1].sum\n",
    "example_doc = train.examples[1].story\n",
    "ex = train.examples[1].sen_vec\n",
    "print(ex)\n",
    "#' '.join(example_doc)\n",
    "print(len(example_q), len(example_sum), len(example_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Voc` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all description &rarr; headline, so if we want to generate text from headline &rarr; description I added the `reverse` flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1105, 100])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = TEXT.vocab\n",
    "vocab.vectors.shape\n",
    "#vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have iter\n",
      "(1435, 100)\n",
      "(1, 1435, 100)\n",
      "(1435, 100)\n",
      "(2, 1435, 100)\n",
      "(1435, 100)\n",
      "(3, 1435, 100)\n",
      "(1435, 100)\n",
      "(4, 1435, 100)\n",
      "(1435, 100)\n",
      "(5, 1435, 100)\n",
      "(1435, 100)\n",
      "(6, 1435, 100)\n",
      "(1435, 100)\n",
      "(7, 1435, 100)\n",
      "(1435, 100)\n",
      "(8, 1435, 100)\n",
      "(1435, 100)\n",
      "(9, 1435, 100)\n",
      "(1435, 100)\n",
      "(1, 1435, 100)\n",
      "(1435, 100)\n",
      "(2, 1435, 100)\n",
      "(1435, 100)\n",
      "(3, 1435, 100)\n",
      "(1435, 100)\n",
      "(4, 1435, 100)\n",
      "(1435, 100)\n",
      "(5, 1435, 100)\n",
      "(1435, 100)\n",
      "(6, 1435, 100)\n",
      "(1435, 100)\n",
      "(7, 1435, 100)\n",
      "(1435, 100)\n",
      "(8, 1435, 100)\n",
      "(1435, 100)\n",
      "(9, 1435, 100)\n",
      "size of batch 9 || len of examples in batch 390\n",
      "Numericalized Batch\n",
      "Variable containing:\n",
      "    2     2     2  ...      2     2     2\n",
      "    4     4     9  ...      4    22    30\n",
      "  506    17    29  ...     83    18    63\n",
      "       ...          ‚ã±          ...       \n",
      "    1     1     1  ...      1     1  1082\n",
      "    1     1     1  ...      1     1   123\n",
      "    1     1     1  ...      1     1     3\n",
      "[torch.LongTensor of size 390x9]\n",
      "\n",
      "Shape of embedded batch\n",
      "torch.Size([390, 9, 100])\n",
      "Sentence vectors batch\n",
      "Variable containing:\n",
      "( 0  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 1  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 2  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      " ... \n",
      "\n",
      "( 6  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0  -2   1  ...    0  -1   0\n",
      "\n",
      "( 7  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 8  ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ‚ã±       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.cuda.LongTensor of size 9x1435x100 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_iterator_over_batches(dataset, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    dataset_iter = data.BucketIterator(\n",
    "        dataset, batch_size=batch_size, device=-1, \n",
    "        sort_key=lambda x: len(x.story),\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    dataset_iter.create_batches()\n",
    "    return dataset_iter\n",
    "\n",
    "#make bucket iterator \n",
    "b_iter = get_iterator_over_batches(train, 32)\n",
    "print('have iter')\n",
    "for batch in b_iter:\n",
    "    print('size of batch' , batch.batch_size, '|| len of examples in batch', len(batch.story))\n",
    "    \n",
    "    # examples come out of the batch numericalized in a batch of (num_words_in_padded_sen X batch_size)\n",
    "    print(\"Numericalized Batch\")\n",
    "    print(batch.story)\n",
    "    \n",
    "    # We can decode an example back into a list of words to see that these values are reasonable\n",
    "    example = batch.story[:,0].data.tolist()\n",
    "    #print('decoded example from batch:')\n",
    "    #print([vocab.itos[e] for e in example])\n",
    "    \n",
    "    # We can create a example embedding layer to see what happens when we use the glove vectors properly\n",
    "    embed = nn.Embedding(len(vocab), 100)\n",
    "    embed.weight.data.copy_(vocab.vectors)\n",
    "    \n",
    "    print(\"Shape of embedded batch\")\n",
    "    embedding = embed(batch.story) # Embedded batch will be (num_words X batch_size X glove_dim)   \n",
    "    print(embedding.shape)\n",
    "    \n",
    "    print(\"Sentence vectors batch\")\n",
    "    print(batch.sen_vec)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning training data into Tensors/Variables\n",
    "\n",
    "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a Tensor, where each word is replaced with the index (from the Lang indexes made earlier). While creating these tensors we will also append the EOS token to signal that the sentence is over.\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)\n",
    "\n",
    "A Tensor is a multi-dimensional array of numbers, defined with some type e.g. FloatTensor or LongTensor. In this case we'll be using LongTensor to represent an array of integer indexes.\n",
    "\n",
    "Trainable PyTorch modules take Variables as input, rather than plain Tensors. A Variable is basically a Tensor that is able to keep track of the graph state, which is what makes autograd (automatic calculation of backwards gradients) possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder will take a batch of word sequences, a `LongTensor` of size `(max_len x batch_size)`, and output an encoding for each word, a `FloatTensor` of size `(max_len x batch_size x hidden_size)`.\n",
    "\n",
    "The word inputs are fed through an [embedding layer `nn.Embedding`](http://pytorch.org/docs/nn.html#embedding) to create an embedding for each word, with size `seq_len x hidden_size` (as if it was a batch of words). This is resized to `seq_len x 1 x hidden_size` to fit the expected input of the [GRU layer `nn.GRU`](http://pytorch.org/docs/nn.html#gru). The GRU will return both an output sequence of size `seq_len x hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        if type(input_seqs) == list:\n",
    "            print('List: ', len(input_seqs))\n",
    "        else:\n",
    "            print('np array', input_seqs.shape)\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Bahdanau et al. model\n",
    "\n",
    "The attention model in [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) is described as the following series of equations.\n",
    "\n",
    "Each decoder output is conditioned on the previous outputs and some $\\mathbf x$, where $\\mathbf x$ consists of the current hidden state (which takes into account previous outputs) and the attention \"context\", which is calculated below. The function $g$ is a fully-connected layer with a nonlinear activation, which takes as input the values $y_{i-1}$, $s_i$, and $c_i$ concatenated.\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "The current hidden state $s_i$ is calculated by an RNN $f$ with the last hidden state $s_{i-1}$, last decoder output value $y_{i-1}$, and context vector $c_i$.\n",
    "\n",
    "In the code, the RNN will be a `nn.GRU` layer, the hidden state $s_i$ will be called `hidden`, the output $y_i$ called `output`, and context $c_i$ called `context`.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of all encoder outputs, where each weight $a_{ij}$ is the amount of \"attention\" paid to the corresponding encoder output $h_j$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "... where each weight $a_{ij}$ is a normalized (over all steps) attention \"energy\" $e_{ij}$ ...\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "... where each attention energy is calculated with some function $a$ (such as another linear layer) using the last hidden state $s_{i-1}$ and that particular encoder output $h_j$:\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing an attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) by Luong et al. describe a few more attention models that offer improvements and simplifications. They describe a few \"global attention\" models, the distinction between them being the way the attention scores are calculated.\n",
    "\n",
    "The general form of the attention calculation relies on the target (decoder) side hidden state and corresponding source (encoder) side state, normalized over all states to get values summing to 1:\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "The specific \"score\" function that compares two states is either *dot*, a simple dot product between the states; *general*, a a dot product between the decoder hidden state and a linear transform of the encoder state; or *concat*, a dot product between a new parameter $v_a$ and a linear transform of the states concatenated together.\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) = v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ]\n",
    "$$\n",
    "\n",
    "The modular definition of these scoring functions gives us an opportunity to build specific attention module that can switch between the different score methods. The input to this module is always the hidden state (of the decoder RNN) and set of encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, input_size, output_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.attn = nn.Linear(self.input_size, output_size)\n",
    "        self.v = nn.Parameter(torch.FloatTensor(1, output_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[b].unsqueeze(0), encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "            \n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.contiguous().view(-1), energy.contiguous().view(-1))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bahdanau et al. model\n",
    "\n",
    "In summary our decoder should consist of four main parts - an embedding layer turning an input word into a vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The decoder's inputs are the last RNN hidden state $s_{i-1}$, last output $y_{i-1}$, and all encoder outputs $h_*$.\n",
    "\n",
    "* embedding layer with inputs $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* attention layer $a$ with inputs $(s_{i-1}, h_j)$ and outputs $e_{ij}$, normalized to create $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* context vector $c_i$ as an attention-weighted average of encoder outputs\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* RNN layer(s) $f$ with inputs $(s_{i-1}, y_{i-1}, c_i)$ and internal hidden state, outputting $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* an output layer $g$ with inputs $(y_{i-1}, s_i, c_i)$, outputting $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.word_attn = Attn('concat', hidden_size * 2, hidden_size)\n",
    "        self.sentence_attn = Attn('concat', sentence_size + hidden_size, hidden_size) # set at start of notebook\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs, sentence_vectors, sentence_idx):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        print(encoder_outputs.size())\n",
    "        print(last_hidden.size())\n",
    "\n",
    "        word_attn_weights = self.word_attn(last_hidden[-1], encoder_outputs)\n",
    "        print(\"Word attention weight shape: \", word_attn_weights.shape)\n",
    "\n",
    "        sentence_attn_weights = self.sentence_attn(last_hidden[-1], sentence_vectors)\n",
    "        print(\"Sentence attention weight shape: \", sentence_attn_weights.shape)\n",
    "\n",
    "        # shouldn't have to clamp, idk why\n",
    "        indices_var = Variable(sentence_idx.unsqueeze(1)\n",
    "                               .transpose(2,0)\n",
    "                               .clamp(0, torch.max(sentence_idx)))\n",
    "        stretched_sent_attn_weights = torch.gather(sentence_attn_weights, 2, indices_var)\n",
    "\n",
    "        attn_weights = F.softmax(word_attn_weights * stretched_sent_attn_weights, dim=2)\n",
    "        print(\"Combined attn weights, \", combined_attn_weights.size())\n",
    "        \n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        context = context.squeeze(0) # B x N\n",
    "\n",
    "        #print(\"Context size: \", context.size())\n",
    "        #print(\"RNN out size: \", output.size())\n",
    "        #print(\"Output cat Context size: \", torch.cat((output, context), 1).shape)\n",
    "        #print(\"Output size: \", self.out(torch.cat((output, context), 1)).shape)\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, word_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models\n",
    "\n",
    "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "small_batch_size = 32\n",
    "b_iter = get_iterator_over_batches(train, small_batch_size)\n",
    "print('have iter')\n",
    "\n",
    "for batch in b_iter:\n",
    "    input_batches = batch.story\n",
    "    input_lengths = len(batch.story)\n",
    "    target_batches = batch.sum\n",
    "    target_lengths = len(batch.sum)\n",
    "    print()\n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        target_batches = target_batches.cuda()\n",
    "    # input_batches, input_lengths, target_batches, target_lengths = random_batch(small_batch_size)\n",
    "\n",
    "    print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
    "    print('target_batches', target_batches.size()) # (max_len x batch_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create models with a small size (a good idea for eyeball inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_hidden_size = 100 #needs to be size of glove vector embeddings?\n",
    "small_n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(len(vocab), small_hidden_size, small_n_layers)\n",
    "decoder_test = BahdanauAttnDecoderRNN(small_hidden_size, len(vocab), small_n_layers)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    decoder_test.cuda()\n",
    "    \n",
    "print(encoder_test)\n",
    "print(decoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the encoder, run the input batch through to get per-batch encoder outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths, None)\n",
    "\n",
    "print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "print('encoder_hidden', encoder_hidden.size()) # n_layers * 2 x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then starting with a SOS token, run word tokens through the decoder to get each next word token. Instead of doing this with the whole sequence, it is done one at a time, to support using it's own predictions to make the next prediction. This will be one time step at a time, but batched per time step. In order to get this to work for short padded sequences, the batch size is going to get smaller each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_target_length = target_lengths\n",
    "\n",
    "# Prepare decoder input and outputs\n",
    "decoder_input = Variable(torch.LongTensor([vocab.stoi['<SOS>']] * small_batch_size))\n",
    "print(decoder_input.size())\n",
    "decoder_hidden = encoder_hidden[:decoder_test.n_layers] # Use last (forward) hidden state from encoder\n",
    "all_decoder_outputs = Variable(torch.zeros(max_target_length, small_batch_size, decoder_test.output_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    decoder_input = decoder_input.cuda()\n",
    "\n",
    "# Run through decoder one time step at a time\n",
    "for t in range(target_lengths):\n",
    "    decoder_output, decoder_hidden, decoder_attn = decoder_test(\n",
    "        decoder_input.contiguous(), decoder_hidden.contiguous(), encoder_outputs.contiguous()\n",
    "    )\n",
    "    all_decoder_outputs[t] = decoder_output # Store this step's outputs\n",
    "    decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    \n",
    "print(all_decoder_outputs.size())\n",
    "print(target_batches.size())\n",
    "\n",
    "# Test masked cross entropy loss\n",
    "loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    target_batches.transpose(0, 1).contiguous(),\n",
    "    torch.LongTensor([vocab.stoi['<SOS>']] * target_batches.size(1)),\n",
    "    use_cuda=USE_CUDA\n",
    ")\n",
    "print('loss', loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder.\n",
    "\n",
    "### Teacher Forcing and Scheduled Sampling\n",
    "\n",
    "\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster, but [when the trained network is exploited, it may exhibit instability](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - you could think of it as having learned how to listen to the teacher's instructions, without learning how to venture out on its own.\n",
    "\n",
    "The solution to the teacher-forcing \"problem\" is known as [Scheduled Sampling](https://arxiv.org/abs/1506.03099), which simply alternates between using the target values and predicted values when training. We will randomly choose to use teacher forcing with an if statement while training - sometimes we'll feed use real target as the input (ignoring the decoder's output), sometimes we'll use the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, sentence_batches, sentence_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "    print('max_target_length = ' + str(max_target_length))\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs, sentence_batches\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "\n",
    "To start, we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 100\n",
    "#batch_size = 50\n",
    "dropout= .1\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(voc.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = BahdanauAttnDecoderRNN(hidden_size, voc.n_words, n_layers)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     decoder = nn.DataParallel(decoder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 50000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 2\n",
    "evaluate_every = 100\n",
    "\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the progress while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 2 # Reset every print_every\n",
    "plot_loss_total = 2 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    evaluate_and_show_attention(pair['story'], pair['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import io\n",
    "# import torchvision\n",
    "# from PIL import Image\n",
    "# import visdom\n",
    "# vis = visdom.Visdom()\n",
    "\n",
    "# def show_plot_visdom():\n",
    "#     buf = io.BytesIO()\n",
    "#     plt.savefig(buf)\n",
    "#     buf.seek(0)\n",
    "#     attn_win = 'attention (%s)' % hostname\n",
    "#     vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    #show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "#     output_words, attentions = evaluate(input_sentence)\n",
    "#     output_sentence = ' '.join(output_words)\n",
    "#     print('>', input_sentence)\n",
    "#     if target_sentence is not None:\n",
    "#         print('=', target_sentence)\n",
    "#     print('<', output_sentence)\n",
    "    \n",
    "#     show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "#     # Show input, target, output text in visdom\n",
    "#     win = 'evaluted (%s)' % hostname\n",
    "#     text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "#     vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "**TODO** Run `train_epochs` for `n_epochs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the train function many times, printing a summary as we go.\n",
    "\n",
    "*Note:* If you're running this notebook you can **train, interrupt, evaluate, and come back to continue training**. Simply run the notebook starting from the following cell (running from the previous cell will reset the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths, sentence_batches, sentence_lengths = random_batch(batch_size)\n",
    "\n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths, sentence_batches, sentence_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer, criterion\n",
    "    )\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "    \n",
    "    print_loss_avg = print_loss_total / print_every\n",
    "    print_loss_total = 0\n",
    "    print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "    print(print_summary)\n",
    "\n",
    "    ##job.record(epoch, loss)\n",
    "\n",
    "#     if epoch % 1 == 0:\n",
    "#         print_loss_avg = print_loss_total / print_every\n",
    "#         print_loss_total = 0\n",
    "#         print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "#         print(print_summary)\n",
    "        \n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        \n",
    "        '''\n",
    "        ecs_win = 'encoder grad (%s)' % hostname\n",
    "        dcs_win = 'decoder grad (%s)' % hostname\n",
    "        vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "        vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "        '''\n",
    "\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\"premier romano prodi battled tuesday for any votes freed up from a split in a far left party but said he will resign if he loses a confidence vote expected later this week .\")\n",
    "plt.matshow(attentions.numpy())\n",
    "show_plot_visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"a south korean lawmaker said friday communist north korea could be producing plutonium and could have more secret underground nuclear facilities than already feared .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"egyptian president hosni mubarak met here sunday with syrian president hafez assad to try to defuse growing tension between syria and turkey .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"police and soldiers on friday blocked off the street in front of a house where members of a terrorist gang are believed to have assembled the bomb that blew up the u .s . embassy killing people .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"premier romano prodi battled tuesday for any votes freed up from a split in a far left party but said he will resign if he loses a confidence vote expected later this week .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To do\n",
    "\n",
    "* Try with a different dataset\n",
    "    * cnn/dailymail\n",
    "    * gigawords\n",
    "    * standford\n",
    "    * Human &rarr; Machine (e.g. IOT commands)\n",
    "    * Chat &rarr; Response\n",
    "    * Question &rarr; Answer\n",
    "* Replace the embedding pre-trained word embeddings such as word2vec or GloVe\n",
    "* Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
    "* Try different RNN layers like lstm.\n",
    "* Add batch operation for GPU training\n",
    "* Add beam search on decoder side when dealing with long documents.\n",
    "* Control the Different output size\n",
    "* Dig out other tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
